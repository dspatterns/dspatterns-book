# Careful Counting Patterns

```{r setup, include=FALSE, echo=FALSE}
library(dspatterns)
library(bakeoff)
library(dplyr)
library(tidyr)
library(skimr)
library(gt)
library(ggplot2)
```

```{css, echo=FALSE}
# TODO: place in external CSS file
pre code {
    overflow-wrap: unset;
    word-break: unset;
    white-space: unset;
}

pre.numberSource code > span > a:first-child::before {
  left: -0.5em;
}

pre.numberSource {
  margin-left: 2em;
}
```

Counting things in our data is super important. It allows us to account for things and also make sense of data. Sometimes you'll want sums of values (for example, total amount of revenue earned in a week or a month), other times, you'll need an accurate count of occurrences (like the daily active users of a service).

We'll make extensive use of the **bakeoff** package. It's a package that contains datasets based on the Great British Bake Off television show. In particular, the `bakers` dataset will be used for many of the examples.

As far as packages requirements, we will need **bakeoff**, **tidyverse**, **gt**, and **pointblank**.

This chapter's pattern:

-   Explore: glimpse / head / tail / skimr
-   Understand:
    -   tables + viz (distinct / count / geom_col janitor::tabyl)
    -   from exploratory plots (confirms or disproves your assumptions about the data) gain an intuition and develop that ability
-   Explain: nrow/ncol + combine with glue for inline code? / epoxy
-   Share (Collaborate/Communicate): use pointblank; gt for tables?

## Explore

We are going to explore a few datasets from the **bakeoff** package. It's primarily a *dataset* package but it has a few handy functions as well. Let's begin with **bakeoff**'s `bakers` dataset. We know nothing about it at this point so we'll use `glimpse()` to inspect the structure of the data table (plus some of row content).

```{r}
glimpse(bakers)
```

Right up front we get told there are 120 rows and 8 columns. That's very useful because we get a sense of the overall size of the dataset. Because `bakers` is a tibble, when the core *Tidyverse* packages are loaded with `library(tidyverse)` we get a pretty nice printing of the table just by calling its name. Check it out:

```{r}
bakers
```

There's no shortage of ways to preview a tabular dataset. We can even use the **gt** package (loaded with `library(gt)`) to give us an HTML-based preview of `bakers`. For that, we use the `gt_preview()` function. Here's how that looks:

```{r}
gt_preview(bakers)
```

<br />

This table presents really nicely and so it's great for sharing a preview of the dataset (e.g., in static R Markdown documents).

Sometimes, however, you need more. You need some stats. Descriptive statistics can better aid in the understanding of a dataset. You'll get a sense of numerical ranges (for numerical columns), if there are missing values in certain columns, and the degree of uniqueness across columns (or variables). There are multiple great solutions for this in **R**. One of them in the `skim()` function provided by the **skimr** package:

```{r}
skimr::skim(bakers)
```

By default, we get a lot of information very quickly. The printed info gives us:

-   the table name and its dimensions (row and column counts)
-   a listing of column types and their frequencies
-   whether there are 'grouped' variables (via dplyr's `group_by()` function)
-   info tables for variables by their type with the degree of missingness (`n_missing`) and the completion rate `complete_rate`; let's look at the specialized bits of info for the three column types we have in `bakers`:
    -   *character*: minimum and maximum string lengths, 'empty' strings (`""`), number of unique strings (`n_unique`), and number of strings that are just whitespace (e.g., `" "`)
    -   *factor*: is the factor `ordered`? How many unique factor levels are there? What are the most frequent levels?
    -   *numeric*: descriptive stats like the mean, the standard deviation, quantiles, *and*, a nice little histogram for a quick visual of the data distribution

The `skim()` function is certainly something to keep in your toolbelt, and quite handy when you get a new dataset to look at for the first time.

Another option is the `scan_data()` function from the **pointblank** package. It's a lot like `skim()` in principle, except it provides even more information and does so in a strictly-HTML-based report. Let's look at `bakers` via `scan_data()`

![Table Scan for bakers](figures/04-careful-counting-patterns/scan-data-bakers.png){#fig:scan-data-bakers}

The report is **huge** and highly interactive. The graphic above shows a bit of what you'll find in the report. The reason this would be useful is that anomalies in the data (especially egregious ones) are easily discovered by looking through the report. For that reason, it's a great idea to use this functionality whenever encountering either

1.  a new dataset
2.  a familiar dataset where data might have changed

You never really know when a dataset may change for the worse, so, scan it up and down with `scan_data()`.

## Understand

To get an even greater understanding of our data, we'll want to ask several questions of it and then get the answer to confirm our assumptions. Let's explore the `bakers` dataset from the **bakeoff** package. This table provides a record for every contestant from every series of the show. In particular, it provides their names (`baker_full`, `baker_last`, and `baker_first`), ages (`age`), occupations (`occupation`), and where they live (`hometown`).

Suppose we have the question: are the contestants unique across all of the rows? Well, we could find that out by knowing that there are `120` rows in the dataset and that using the `distinct()` function (from the **dplyr** package) should give us the same number of rows when used just on the `baker_full` column (full names for each contestant). Let's take a look at that in action:

```{r, paged.print=FALSE}
bakers %>% distinct(baker_full)
```

Yes! This one-column table gives us 120 rows as well. This proves that every contestant appeared once per season. If a person appeared in multiple seasons then the resulting table would have less than 120 rows (since duplicate entries for `baker_full` would be removed).

Here's another question: what's the most common first name for a contestant on the show? To get this, we'd have to focus on the `baker_first` column (first names for each and every contestant).

```{r, paged.print=FALSE}
bakers %>% distinct(baker_first)
```

The row count here is `107` and this is less than the `120` row count for the complete dataset. From this, we now know there are some first names shared amongst the contestants. Let's change up our strategy and instead use the `count()` function (from **dplyr**) to count the first names (in the `baker_first` column):

```{r, paged.print=FALSE}
bakers %>% count(baker_first)
```

The table we get now has counts that go along with each of the first names! But the names are sorted alphabetically and it's hard to determine what the most common name is. This can be entired remedied by using the `sort` option in `count()`. With `sort = TRUE`, the sorting of the `n` column will be from high to low.

```{r, paged.print=FALSE}
bakers %>% count(baker_first, sort = TRUE)
```

This is exactly what we need and it's now plain to see that the most common first name is Kate (with three *different* contestants having that name). Should you want to visualize this, it can be done very quickly with **ggplot** and the `geom_col()` function. Here, we will do that, but only include those names that are shared with others.

```{r}
bakers %>%
  count(baker_first, sort = TRUE) %>%
  filter(n > 1) %>%
  ggplot() +
  geom_col(aes(x = n, y = baker_first))
```

On to another question we might ask of the dataset: was there any series where two contestants shared the same first name? We can find that out by getting a count of contestants' first names by series. Still using `count()` to do this except now we must provide *two* grouping variables (`baker_first` and `series`).

```{r, paged.print=FALSE}
bakers %>% count(series, baker_first, sort = TRUE)
```

It seems like this is not the case. All `n` values are `1` and the total row count in this output table is the same as that of the original table (`120`). This follows the original order of the table (series increasing). Note that if you reverse the groups (specifying `baker_first` and then `series`) then the sorting order becomes an alphabetical sorting of all bakers' first names (since `n` is `1` all the way down).

Can we figure out if any bakers are from the same hometown? How about from the same hometown and the same series? Sure. Use the groups `hometown` and `series` and then ensure that `sort = TRUE` as before.

```{r, paged.print=FALSE}
bakers %>% count(hometown, series, sort = TRUE)
```

We can see that we had fellow townies in series `8` and four Londoners in series `9`. For sure, this `count()` function from **dplyr** is incredibly powerful.

We can extend this line of thinking to get a count of contestants per episode across all series. Again, `count()` is your friend here and all we need to do is choose the right grouping variables. In this case, those variables are `series` and `episode`. Here it is:

```{r, paged.print=FALSE}
episodes %>% count(series, episode)
```

We see that the counts of contestants across episodes in each series decreases, which is to be expected from shows where participants are routinely eliminated. This is plottable and, crucially, we can facet the plots by `series`.

```{r}
episodes %>% 
  count(series, episode) %>%
  ggplot() +
  geom_col(aes(x = episode, y = n)) +
  facet_wrap(~series)
```

This is an interesting overview of the series and how eliminations were carried out. We see that most series had 10 episodes except for the first two. Most episodes resulted in the elimination of a single contestant but, sometimes, it was two (or none!).

## Explain

Now that we better understand the **bakeoff** data in general. We need to develop some ways to explain the data. If you're using R Markdown or Quarto, you'll sometimes want to get the values inside of the inline text of the document. This is key for describing your data and we can, for example, use `nrow()` to get the number of rows in the table like this `r knitr::inline_expr("nrow(bakers)")`. It's pretty essential that we directly use the input data and function calls to get the derived values. If the values were hardcoded that might be fine but there's always the risks that (1) your transcription is inaccurate, and (2) the underlying data might change when rendering the document.

While sometimes it's useful to explain things in the prose of the report, other times a well-placed table can provide more information. Let's look at a few summary tables that can explain the results found previously.

Perhaps the last plot of contestant counts per episode per series might be better presented as a table? Sometimes it's hard to really know until the table is made. We can do that with `count()` as before, pivoting the data to a wide format with `pivot_wider()` (so that we can scan across episodes per each series), and then use `gt()` to construct the summary table.

```{r}
episode_summary <- 
  episodes %>% 
  count(series, episode) %>%
  pivot_wider(names_from = episode, values_from = n) %>%
  gt(rowname_col = "series") %>%
  tab_spanner(label = "Episode", columns = matches("[0-9]*")) %>%
  fmt_missing(columns = everything()) %>%
  cols_width(1 ~ "60px", everything() ~ "40px") %>%
  tab_stubhead(label = "Series")

episode_summary
```

This is good and presents exact numbers per episode (important if someone needs to scan down episode numbers across seasons and compare contestant counts). In this summary table we always see that the last episode became a showdown between three final contestants and other little patterns become apparent, like that 5-4-3 contestants is a motif across all the series.

## Share

pointblank::scan_data() pointblank::draft_validation_report()

-   This chapter really ought to be an intro vis / data inspection/exploration section with multiple packages

-   use some code from course material that uses bakeoff

    -   links: https://alison.netlify.app/rls-plot-twist/#19, https://alison.netlify.app/rls-plot-twist/#19 (counting slides)

-   start off with histogram and create this plot: https://alison.netlify.app/rls-plot-twist/#19

-   histogram of ages of bakers + other interesting plots

-   goals: making exploratory bar charts and histograms (useful for initial understanding of the data)

## Project Ideas

(should have a fully working set of Rmd templates, presented in the form of a project template - this can be prepped for the reader with a function call)

The moma dataset (https://apreshill.github.io/data-vis-labs-2018/02-moma.html) would be great for a project - explore using the lisa palette from paletteer

### Packages Needed

-   installation details (script with `library()` calls)
-   possibly use: https://github.com/rstudio4edu/rmd4edu/blob/master/R/lesson.R

## DIY

-   dataset:

---
title: "Importing Data the Next Time"
---

{{< include ../_patterns.qmd >}}

```{r setup, include=FALSE, echo=FALSE}
library(dspatterns)
library(intendo)
library(tidyverse)
library(pointblank)
```

*There will always be a next time*. This statement very much holds true when it comes to routinely loading in data for analysis. This variation of data loading is different from the *first time*, where you have no familiarity with the data at hand. Rather, this is data that perhaps comes in regularly (through updates) and the structure of it doesn't really change (or at least, it *shouldn't*).  

This pattern is really different because we're not coming to grips with what the data is. It's a known quantity by now and so the concerns are different. One of these is: "Will this updated dataset work with my existing scripts and workflows?". Typically, a workflow will adapt or scale with data that's an expansion of earlier, working data (in a similar fashion to subsets of data). So the main concern is less about understanding the data and generating the workflows around the data but ensuring that data quality is maintained with subsequent updates.

We'll need a few packages loaded in for this chapter:

- **tidyverse**: for various functions across a collection of packages (the Tidyverse packages)
- **intendo**: for the dataset we'll be using (`user_summary`)
- **pointblank**: for building a data dictionary and performing data checks

## Explore

Before we dive right in and look at a messed up table (full of data quality issues) let enumerate the types of data quality problems that happen on a regular basis.

**Missing Values**

Missing values occur when there are empty or null entries in the data. Missing values can lead to biased or incomplete analysis since they can introduce inconsistencies and distort statistical calculations. Analysts may need to handle missing values appropriately by imputing them or excluding them from the analysis, depending on the context.

**Inconsistent Formatting**

Inconsistent formatting refers to variations in how data is represented, such as inconsistent date formats or inconsistent capitalization. Inconsistent formatting can make data difficult to analyze, compare, or merge across different datasets. It can also lead to errors in calculations or misinterpretations of the data.

**Duplicates**

Duplicates occur when there are multiple identical or similar entries in the dataset. Duplicates can distort statistical analysis, as they may artificially inflate counts, averages, or other summary metrics. They can also lead to incorrect conclusions or duplicate efforts if not properly identified and handled.

**Outliers**

Outliers are extreme values that deviate significantly from the majority of the data. Outliers can have a disproportionate impact on statistical measures, potentially skewing results or leading to incorrect assumptions. Analysts need to identify and evaluate outliers to determine their validity and decide whether to include, transform, or remove them from the analysis.

**Inaccurate or Incomplete Data**

Inaccurate or incomplete data refers to data that contains errors, inaccuracies, or omissions. This can include data entry errors, measurement errors, or incomplete records. Inaccurate or incomplete data can lead to incorrect analysis, biased conclusions, or unreliable insights. It is crucial for analysts to validate and verify data quality and address any inaccuracies or incompleteness before drawing conclusions.

This is just a sampling of data quality issues someone dealing with data might face. Data quality issues are straight up problematic for data people. These problems harm the integrity, reliability, and validity of analytical outcomes. This potentially leads to: (1) erroneous insights, (2) biased decision-making, and (3) flawed conclusions; three things which are pretty much the same w.r.t. badness and unwantedness. It is imperative, crucial even, to address data quality problems. This is done of course with data validation, and appropriate handling to safeguard the accuracy and dependability of analyses (and those decision-making processes).

Now to the data. We're going to get that data from the **intendo** package. This data package contains synthetic datasets and it comes from the videogame analytics world. One thing that's really hard to find is data that is purposefully full of errors. The **intendo** package *has* these kind of datasets and, additionally, the errors are fully documented (in the package codebase, that is). But don't cheat and take a look just yet! We need to discover what the problems are through data quality checks.

The dataset we are going to use for our data-quality-checking examples is called `user_summary` and we can access it (and choose the `"faulty"` variant) through **intendo**'s `user_summary()` function.

```{r}
#| output: false
#| echo: false
user_summary <- user_summary(quality = "faulty")
```

```r
user_summary <- user_summary(quality = "faulty")
```

Let's look at the faulty `user_summary` table using **dplyr**'s `glimpse()` function:

```{r}
glimpse(user_summary)
```

This dataset, unlike the previously used `stickers` dataset, has no weirdness with regard to overly-long column names or the like. The major malfunction here is the data itself: it's faulty. We obviously can't see that in the `glimpse()` of the data but it's there. We have to deal with this using a high amount of data quality checks. Small errors can easily creep in and cause small to huge problems with your analysis, so, this is super important!

Let's look at the data and what it means. The **intendo** package smartly provides data dictionaries for each of its datasets. We can view the data dictionary for the `user_summary` table with the 
`user_summary_dd()` function.

```r
user_summary_dd()
```

That data dictionary is pretty detailed! It doesn't provide sample values for any column, but, it does describe the data reasonably well (especially the `player_id` column). This should be read over at least twice and then, once we have a solid understanding of the data again (supposing we've seen this data before) we can then get to exploring the data again and building some data quality checks we can always run when we get refreshed data. 

## Explore

Let's use `scan_data()` to look at the `user_summary` table. In this case, we only want to look at a few sections of the full data scan.

```r
scan_data(user_summary, sections = "OVMS")
```

The table is not supposed to have missing values, but this version of `user_summary` has 26 of them. That bit of information is shown in the *Table Overview* section of the table scan.

![Part of the table scan, where we get an overview of the table and its columns.](img/table-scan-01.png)

That already is bad data detected through a useful summary. By moving down to the *Missing Values* section we see that the missing data is entirely found in the `country` and `acquisition` columns. (This is the sort of good information to have when speaking to your data provider; they can potentially fix the problem.)

![Part of the table scan, where we see if and where data is missing.](img/table-scan-02.png)

The next thing to do is look at the summary details for each of the columns. For `player_id`, we seem to have completely distinct values, and, that's expected (the data dictionary says they should be unique). The string lengths are always 15 characters, so, that's a good sign as well. We *don't* get information in the table scan that tells us whether the IDs are formed as expected from the data dictionary ("12 uppercase letters followed by 3 digits"). We'll check for that later.

The next two columns (`first_login` and `start_day` columns) are `"datetime"`- and `"date"`-type columns. This is corroborated in the data dictionary. There's nothing yet to be suspicious about for these two columns.

The `country` column, which we already know is problematic since is has `NA` values, should logically contain country names. The data dictionary doesn't say much about how country names are represented in the data. In the *Common Values* subsection of the table scan for `country`, we say common values written as `"United States"`, `"China"`, `"France"`, etc. Not the greatest practice, since there are more standardized ways of identifying a country. When looking at the *String Lengths* subsection we can see something a bit odd: minimum string values of `2`. Since no country names written in English are as small as two characters, something else is amiss. We'll check this further!

![Part of the table scan, showing the proportions of string lengths for text in the `country` column.](img/table-scan-03.png)

For the `acquisition` column, aside from the `NA` values that ought not to be there, there seems to be nothing out of the ordinary when drilling down into that column's summary values.

The `device_name` column looks fine, and the device names seem to be written out in a semi-standardized style. There are literally hundreds of distinct values here so we have to see if the formatting convention holds.

That concludes our scan of the table scan! We got some leads from it and we'll be extra sure to check for those suspected problems and look for data anomalies. It's a good idea always to save the report to disk using the `export_report()` function. Such table scan snapshots are useful to have around while formulating data quality checks.

## Understand

One really cool thing you can do with **pointblank** (the package that's all about data quality checks) is use a function called `draft_validation()`. Executing that with the dataset gives you an `.R` script with a ton of data quality statements written for you! You run it like this

```{r}
#| output: false
#| echo: false
#| eval: false

draft_validation(
  tbl = user_summary,
  tbl_name = "User Summary",
  file_name = "user_summary_validation"
)
```

and you get a .R file that looks like this

```r
library(pointblank)

agent <-
  create_agent(
    tbl = ~ user_summary,
    actions = action_levels(
      warn_at = 0.05,
      stop_at = 0.10
    ),
    tbl_name = "User Summary",
    label = "Validation plan generated by `draft_validation()`."
  ) %>%
  # Expect that column `player_id` is of type: character
  col_is_character(
    columns = vars(player_id)
  ) %>%
  # Expect that column `country` is of type: character
  col_is_character(
    columns = vars(country)
  ) %>%
  # Expect that values in `country` should be in the set of `NA`, `United States`, `Japan` (and 27 more)
  col_vals_in_set(
    columns = vars(country),
    set = c(NA, "United States", "Japan", "Switzerland", "China", "Denmark", "Hong Kong", "South Africa", "United Kingdom", "Austria", "India", "Portugal", "Russia", "Philippines", "South Korea", "Spain", "Germany", "Canada", "Egypt", "France", "Norway", "Mexico", "DK", "DE", "PH", "HK", "Australia", "Sweden", "CH", "NO")
  ) %>%
  # Expect that column `acquisition` is of type: character
  col_is_character(
    columns = vars(acquisition)
  ) %>%
  # Expect that column `device_name` is of type: character
  col_is_character(
    columns = vars(device_name)
  ) %>%
  # Expect entirely distinct rows across all columns
  rows_distinct() %>%
  # Expect that column schemas match
  col_schema_match(
    schema = col_schema(
      player_id = "character",
      first_login = c("POSIXct", "POSIXt"),
      start_day = "Date",
      country = "character",
      acquisition = "character",
      device_name = "character"
    )
  ) %>%
  interrogate()

agent
```

This 'draft validation' is named as such because it gives you a starting point that isn't necessarily correct (i.e., the parameters will need tweaking to fit the notions of valid data values), but, it will completely succeed. The underlying routine assumes all data values are *correct* and the validation statements are fit to the supplied data.

Let's copy these agent-based workflow statements (removing the comments). A few additions should be added:

1. all rows should be complete
2. all columns should not have any `NA` values
3. add check for country names
4. ensure that `player_id` values are well formed
5. check that dates/datetimes represent occurrences in `2015`

On point 3, recall that the country names were suspect? Looking at the `set` value used in the `col_vals_in_set()` statement, we can see that there are a few two-letter country names in the mix (that's simply not right). We'll have to change the way we check that column. Maybe the most prudent way to check for this error happening in the future is to check for a string length of at least four (this could guard against three-letter country codes erroneously entered in this column). Really, a closed set may not be ideal since more countries may enter the column on a data refresh/update (and that's valid).

Here's the complete validation plan, including the **pointblank** interrogation:

```{r}
agent <-
  create_agent(
    tbl = ~ user_summary,
    actions = action_levels(warn_at = 1),
    tbl_name = "User Summary",
    label = "Validation of `user_summary` table."
  ) |>
  col_is_character(
    columns = vars(player_id)
  ) |>
  col_vals_regex(
    columns = vars(player_id),
    regex = "[A-Z]{12}[0-9]{3}"
  ) |>
  col_is_posix(columns = vars(first_login)) |>
  col_is_date(columns = vars(start_day)) |>
  col_vals_equal(
    columns = vars(first_login),
    value = "2015",
    preconditions = function(x) {
      dplyr::mutate(x, first_login = substr(first_login, 1, 4))
    }
  ) |>
  col_vals_equal(
    columns = vars(start_day),
    value = "2015",
    preconditions = function(x) {
      dplyr::mutate(x, start_day = substr(start_day, 1, 4))
    }
  ) |>
  col_is_character(columns = vars(country, acquisition, device_name)) |>
  col_vals_gte(
    columns = vars(country),
    value = 4,
    preconditions = function(x) dplyr::mutate(x, country = nchar(country))
  ) |>
  rows_distinct() |>
  rows_complete() |>
  col_vals_not_null(columns = everything()) |>
  col_schema_match(
    schema = col_schema(
      player_id = "character",
      first_login = c("POSIXct", "POSIXt"),
      start_day = "Date",
      country = "character",
      acquisition = "character",
      device_name = "character"
    )
  ) |>
  interrogate()
```

We can print the agent report by calling the `agent` object.

```r
agent
```

![The agent report. It's big and that's because we validated really hard.](img/validation-01.png)

The validation report is a damning indictment of the data within `user_summary`. We can identify four separate data issues with this report. It's possible to isolate only the problematic steps

```r
agent |> get_agent_report(keep = "fail_states")
```

![The part of the agent report that tells us what's not so good.](img/validation-02.png) 

There are issues with the `first_login` datetime values (16 units failing), the values within `country` (47 failures), and the `country` and `acquistion` columns have `NA`s (17 and 9, respectively). Knowledge of data being faulty and knowing *where* it is faulty is quite valuable. Only then will you have a chance to rectify the data generation process or at least know which portions of the data can be less trusted. 

It's good to know if there are upstream processes you don't have access to when it comes to the data you're given. If it's truly raw data, then you can expect that the existing data won't change (new data might be added but existing data won't be mutated). However, if this data has been subject to treatments before being given to you, then that process is a bit out of your hands. However, you can improve these processes by checking data and pointing out anomalies that your data provider might have missed (or didn't check for). In doing so, the provider might improve the data transformation process and you get better (i.e., less anomalous) data.

## Explain

We ought to get a workflow in order to continually test datasets of this type. The common scenario is that the table should data much the same in terms of shape, structure, and intent, but it will be updated with new data and older records may be updated/corrected as well. Because we now have a **pointblank** validation plan that successfully ran with `interrogate()`, we could use that every time the data is updated.

An R script to set this all up might look like this:

```r
library(tidyverse)   # for data manipulation functions
library(pointblank)  # for performing data quality checks
library(intendo)     # for the data itself

# Obtain the updated data
user_summary <- user_summary(quality = "faulty")

# Get the current date and time
report_time <- Sys.time()

# Create a pointblank agent, include several validation steps
# to exhaustively test the data, and interrogate the data
# with the `interrogate()` function
agent <-
  create_agent(
    tbl = ~ user_summary,
    actions = action_levels(warn_at = 1),
    tbl_name = "User Summary",
    label = paste("Validation of `user_summary` table on", report_time)
  ) |>
  col_is_character(
    columns = vars(player_id)
  ) |>
  col_vals_regex(
    columns = vars(player_id),
    regex = "[A-Z]{12}[0-9]{3}"
  ) |>
  col_is_posix(columns = vars(first_login)) |>
  col_is_date(columns = vars(start_day)) |>
  col_vals_equal(
    columns = vars(first_login),
    value = "2015",
    preconditions = function(x) {
      dplyr::mutate(x, first_login = substr(first_login, 1, 4))
    }
  ) |>
  col_vals_equal(
    columns = vars(start_day),
    value = "2015",
    preconditions = function(x) {
      dplyr::mutate(x, start_day = substr(start_day, 1, 4))
    }
  ) |>
  col_is_character(columns = vars(country, acquisition, device_name)) |>
  col_vals_gte(
    columns = vars(country),
    value = 4,
    preconditions = function(x) dplyr::mutate(x, country = nchar(country))
  ) |>
  rows_distinct() |>
  rows_complete() |>
  col_vals_not_null(columns = everything()) |>
  col_schema_match(
    schema = col_schema(
      player_id = "character",
      first_login = c("POSIXct", "POSIXt"),
      start_day = "Date",
      country = "character",
      acquisition = "character",
      device_name = "character"
    )
  ) |>
  interrogate()

# Customize the reporting of the agent by adding
# a title and by arranging the validation steps
# in order of 'severity' (steps with failing units
# are shown first)
agent_report <- 
  get_agent_report(
    agent,
    title = "Data Quality Report",
    arrange_by = "severity"
  )

# Save the agent reporting as an HTML file on disk;
# include the date and time of running the validation
# checks so that we have an ongoing record that sorts
# nicely in a directory
export_report(
  agent_report,
  filename = affix_datetime(
    "user_summary-data-quality-report.html"
  )
)

# Write a YAML file that captures all of the
# data validation instructions in YAML form
yaml_write(
  agent,
  filename = affix_datetime(
    "user_summary-data-quality-inputs.yml"
  )
)
```

We did a few extra things in this script in order to have some additional file artifacts stored in the working directory. First, we customized the agent report with the `get_agent_report()` function (and we assigned it to a new variable called `agent_report`). We can export that to a standalone HTML file with **pointblank**'s `export_report()` function (and we did, in the script above). We also did an extra thing which was to write a YAML file containing all of the **pointblank** validation instructions. This is a great-to-have bit of record-keeping for your data validation pattern. Within the script, we made sure to document the intent of each statement (with comments located above the relevant statements).

This `.R` script (could be a `.qmd`: your choice) organizes this workflow so you can easily keep on top of anything that may compromise data quality for this dataset. If you have multiple datasets to check, they can be part of the same `.R`/`.qmd` and it could be run once (checking lots more data, generating way more files). With what is currently set up here, we can react quickly to data problems and provide some evidence of the data issues to data providers/producers, thanks to the artifacts generated by the script.

We also made sure that the reporting is organized with timestamps in as many places as possible. You might imagine a situation where the data quality checking is running every day on many datasets. With a huge volume of files, we really need an audit trail that clearly identifies the date and time of each check. The filenames have this datetime information (the `affix_datetime()` utility function in **pointblank** makes this easy) and the HTML reports each have the date and time embedded within (top and bottom of each!). The last thing you want is a disorganization of artifacts but the strategy laid out here is a good one to follow.

## Share

- Put extensive notes about each validation step in the data dictionary, or, in an associated report

- Show how to update a data dictionary with important information about the evolution of a dataset

- We have to be able to determine whether data quality is becoming more of a problem or is getting better because that will inform our next actions

This may require some communication with the data provider. For now, we just have to make note of all the changes whether they are correct or not. 

A data dictionary as produced by **pointblank** can serve as really important data documentation for the team. While it can provide the basic information about a dataset (and can be kept up to date with changes to the dataset), it can also serve as a log for more detailed changes to the data. 

- demonstrate how notes can be added to the data dictionary, with dates and detailed text

It's a good idea always to keep the the data dictionary reporting accessible via a persistent link that stakeholders can always access. The report always contains the last modified date at the bottom, so viewers will be assured that they have the latest version in front of them.

Share data quality reports with team/collaborators. These are meant to be easy to read and, again, they should be centralized and available at a link that all stakeholders can access. You may need to explain the checks in some detail with the stakeholders (especially if there are lots of detailed validations). A great place to put descriptions of checks is in the data dictionary. Here's an example of a validation report and the corresponding data dictionary:

<!-- validation report -->

<!-- data dictionary -->

Triage upstream data quality issues with collaborators or data providers and provide descriptions of problems via explanations of 'bad rows' in R Markdown reports (use gt tables to produce data extracts in the document). This communication will help to get to the root causes of the bad data, even if you can't directly do much about it (i.e., collaborator must fix upstream process).

- Demonstrate how problem rows can be extracted and packaged up for communication with upstream data providers

- Save the validation report if the data is changing quickly over time (this can be considered a validation snapshot, important if we have frequent changes)

- What is the frequency and breadth of problems? Is it just one or two that cause predictable errors or are there several that cause many problems?

